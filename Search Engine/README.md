<b>Building an intelligent search engine</b>
-------------------
<ol>
    <li>The source code is developed in Python version 3.7.1. Please ensure that the corresponding version is available to execute the source code.</li>
    <li>The following package dependencies are to be fulfilled in order to execute the code.
        <ul>
            <li>'os': for OS related operations</li>
            <li>'sys': for system related operations</li>
            <li>'hashlib': to hash URLs</li>
            <li>'datetime': to record date and time</li>
            <li>'urllib': to download web pages</li>
            <li>'ssl': to handle SSL operations</li>
            <li>'string': for string operations</li>
            <li>'nltk': for text processing</li>
            <li>'bs4': for HTML processing</li>
            <li>'json': to store extracted contents as JSON files</li>
            <li>'math': to perform math operations</li>
            <li>'gensim': for topic modeling (LDA)</li>
            <li>'logging': for logging information</li>
            <li>'pickle': to persist generated models</li>
            <li>'tkinter': to construct GUI</li>
        </ul>
    </li>
    <li>The search engine contains the following source code files:
        <ul>
            <li>crawler.py</li>
            <li>html_parser.py</li>
            <li>indexer.py</li>
            <li>lda.py</li>
            <li>logger_handler.py</li>
            <li>pagerank.py</li>
            <li>persistence_handler.py</li>
            <li>query_handler.py</li>
            <li>startup.py</li>
            <li>text_processor.py</li>
        </ul>
    </li>
    <li>Pre-requisites:
        <ul>
            <li>In order to launch the search engine, please extract the contents of the Zip folder. Do not change the directory hierarchy.
            </li>
            <li>On startup, the engine tries to load the pre-generated models from the 'data' directory. 
                <ul>
                    <li>indexer - contains the inverted index for the vector space model.</li>
                    <li>network_graph - contains the web graph with the PageRank scores.</li>
                    <li>lda_id2word.gensim, lda_model.gensim, lda_model.gensim.expElogbeta.npy, lda_model.gensim.id2word, lda_model.gensim.state - contains the LDA model components, used for query expansion.</li>
                </ul>
            </li>
            <li>If the user is prompted to crawl, or if the user wishes to crawl a different set of web pages, it can be configured in the 'root_config.txt' file in the 'config' directory.
            </li>
        </ul>
        <li><b>Launching the search engine:</b>
            <ul>
                <li>The search engine can be launched by executing the 'startup.py' file.
                    <br>
                    >> python startup.py
                </li>
                <li>The user will be prompted to choose between accessing a Graphical User Interface and the command prompt.</li>
                <li>The results will be populated in the user view.</li>
            </ul>
        </li>
    </li>
</ol>
<b>Notes:</b>
<ul>
    <li>If the LDA files are not available, the engine would not perform query expansion. However, if the indexer and network_graph are unavailable, the engine would prompt the end user to crawl the web and generate the files.</li>
    <li>Please note that the crawling operation is a time intensive operation a may take a long while to complete.</li>
    <li>On adding/modifying the 'root_urls' property in the configuration file, please mention the full URL.
        <br>
        Ex: http://cs.uic.edu
        <br>
        URLs without their scheme (HTTP/HTTPS) will be ignored by the crawler.
    </li>
    <li>The pre-generated models were generated by crawling 10,000 pages. This is controlled by the 'max_crawl' parameter in the 'root_config.txt' file.</li>
    <li>Launch the engine by traversing to the 'source_code' directory and executing the file directly. If the file is executed from a different directory, the file hierarchy will be lost and thus the engine may not be able to locate the required files.</li>
    <li>During the execution, the search engine will populate logs under the 'logs' directory. This is an expected behavior.</li>
</ul>
