Title:
Building an intelligent search engine
Author:
Sivaraman Lakshmipathy
-------------------

Readme:
1. The source code is developed in Python version 3.7.1. Please ensure that the corresponding version is available to execute the source code.
2. The following package dependencies are to be fulfilled in order to execute the code.
    a) Package 'os': for OS related operations
    b) Package 'sys': for system related operations
    c) Package 'hashlib': to hash URLs
    d) Package 'datetime': to record date and time
    e) Package 'urllib': to download web pages
    f) Package 'ssl': to handle SSL operations
    g) Package 'string': for string operations
    h) Package 'nltk': for text processing
    i) Package 'bs4': for HTML processing
    j) Package 'json': to store extracted contents as JSON files
    k) Package 'math': to perform math operations
    l) Package 'gensim': for topic modeling (LDA)
    m) Package 'logging': for logging information
    n) Package 'pickle': to persist generated models
    o) Package 'tkinter': to construct GUI
3. The search engine contains the following source code files:
    a) crawler.py
    b) html_parser.py
    c) indexer.py
    d) lda.py
    e) logger_handler.py
    f) pagerank.py
    g) persistence_handler.py
    h) query_handler.py
    i) startup.py
    j) text_processor.py
4. Pre-requisites: 
    a) In order to launch the search engine, please extract the contents of the Zip folder. 
    NOTE:   Do not change the directory hierarchy.
    b) On startup, the engine tries to load the pre-generated models from the 'data' directory. 
        i.   indexer - contains the inverted index for the vector space model.
        ii.  network_graph - contains the web graph with the PageRank scores.
        iii. lda_id2word.gensim, lda_model.gensim, lda_model.gensim.expElogbeta.npy, lda_model.gensim.id2word, lda_model.gensim.state - contains the LDA model components, used for query expansion.
    If the LDA files are not available, the engine would not perform query expansion. However, if the indexer and network_graph are unavailable, the engine would prompt the end user to crawl the web and generate the files.
    NOTE: PLEASE NOTE THAT THE CRAWLING OPERATION IS A TIME INTENSIVE OPERATION AND MAY TAKE A LONG WHILE TO COMPLETE!
    c) If the user is prompted to crawl, or if the user wishes to crawl a different set of web pages, it can be configured in the 'root_config.txt' file in the 'config' directory.
    NOTE:   On adding/modifying the 'root_urls' property in the configuration file, please mention the full URL.
            Ex: http://cs.uic.edu
            URLs without their scheme (HTTP/HTTPS) will be ignored by the crawler.
    NOTE:   The pre-generated models were generated by crawling 10,000 pages. This is controlled by the 'max_crawl' parameter in the 'root_config.txt' file.
5. Launching the search engine:
    a) The search engine can be launched by executing the 'startup.py' file.
        >> python startup.py
    NOTE:   Launch the engine by traversing to the 'source_code' directory and executing the file directly. If the file is executed from a different directory, the file hierarchy will be lost and thus the engine may not be able to locate the required files.
    b) The user will be prompted to choose between accessing a Graphical User Interface and the command prompt.
    c) The results will be populated in the user view.
    NOTE:   During the execution, the search engine will populate logs under the 'logs' directory. This is an expected behavior.
-------------------
